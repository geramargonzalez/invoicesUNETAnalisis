{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af572c0",
   "metadata": {},
   "source": [
    "# Invoice Field Detection — U-Net / Deep Learning\n",
    "\n",
    "Este notebook implementa un pipeline completo de detección de campos en facturas/recibos.  \n",
    "El dataset contiene:\n",
    "- **`data/img/`** — imágenes de recibos (626 JPGs)\n",
    "- **`data/box/`** — anotaciones de bounding boxes (CSV: `x1,y1,x2,y2,x3,y3,x4,y4,texto`)\n",
    "- **`data/key/`** — etiquetas de campos clave (JSON: `company`, `date`, `address`, `total`)\n",
    "\n",
    "**Objetivo:** Localizar y clasificar los campos clave (`company`, `date`, `address`, `total`) dentro de cada imagen de recibo mediante segmentación semántica con U-Net."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc4aea6",
   "metadata": {},
   "source": [
    "## 1. Importar Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49bda3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU disponible: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd7c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Rutas del dataset ───────────────────────────────────────────────────────\n",
    "BASE_DIR  = os.path.dirname(os.path.abspath(\"invoice_field_detection.ipynb\"))\n",
    "DATA_DIR  = os.path.join(BASE_DIR, \"data\")\n",
    "IMG_DIR   = os.path.join(DATA_DIR, \"img\")\n",
    "BOX_DIR   = os.path.join(DATA_DIR, \"box\")\n",
    "KEY_DIR   = os.path.join(DATA_DIR, \"key\")\n",
    "\n",
    "# ─── Hiperparámetros ──────────────────────────────────────────────────────────\n",
    "IMG_H, IMG_W = 512, 512          # Tamaño de entrada al modelo\n",
    "NUM_CLASSES  = 5                 # background, company, date, address, total\n",
    "BATCH_SIZE   = 8\n",
    "EPOCHS       = 30\n",
    "SEED         = 42\n",
    "\n",
    "# Mapa de campo → clase\n",
    "FIELD_CLASSES = {\"background\": 0, \"company\": 1, \"date\": 2, \"address\": 3, \"total\": 4}\n",
    "CLASS_COLORS  = {0: (0,0,0), 1: (255,0,0), 2: (0,255,0), 3: (0,0,255), 4: (255,255,0)}\n",
    "CLASS_NAMES   = [\"background\", \"company\", \"date\", \"address\", \"total\"]\n",
    "\n",
    "print(\"Rutas configuradas correctamente.\")\n",
    "print(f\"  IMG_DIR : {IMG_DIR}\")\n",
    "print(f\"  BOX_DIR : {BOX_DIR}\")\n",
    "print(f\"  KEY_DIR : {KEY_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64b0c5",
   "metadata": {},
   "source": [
    "## 2. Cargar y Explorar el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5312afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample(idx: str):\n",
    "    \"\"\"\n",
    "    Carga imagen, bounding boxes y etiquetas clave para un índice dado.\n",
    "    El CSV tiene columnas: x1,y1,x2,y2,x3,y3,x4,y4,text  (sin encabezado)\n",
    "    El JSON contiene: company, date, address, total\n",
    "    \"\"\"\n",
    "    img_path = os.path.join(IMG_DIR, f\"{idx}.jpg\")\n",
    "    box_path = os.path.join(BOX_DIR, f\"{idx}.csv\")\n",
    "    key_path = os.path.join(KEY_DIR, f\"{idx}.json\")\n",
    "\n",
    "    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    boxes_df = pd.read_csv(box_path, header=None,\n",
    "                           names=[\"x1\",\"y1\",\"x2\",\"y2\",\"x3\",\"y3\",\"x4\",\"y4\",\"text\"])\n",
    "\n",
    "    with open(key_path) as f:\n",
    "        keys = json.load(f)\n",
    "\n",
    "    return img, boxes_df, keys\n",
    "\n",
    "\n",
    "# ─── Listar todos los índices disponibles ────────────────────────────────────\n",
    "all_indices = sorted([\n",
    "    os.path.splitext(os.path.basename(p))[0]\n",
    "    for p in glob.glob(os.path.join(IMG_DIR, \"*.jpg\"))\n",
    "])\n",
    "\n",
    "print(f\"Total de muestras: {len(all_indices)}\")\n",
    "print(f\"Primeros índices  : {all_indices[:5]}\")\n",
    "print(f\"Últimos índices   : {all_indices[-5:]}\")\n",
    "\n",
    "# ─── Estadísticas básicas ─────────────────────────────────────────────────────\n",
    "img_sample, boxes_sample, keys_sample = load_sample(all_indices[0])\n",
    "print(f\"\\nEjemplo índice 000:\")\n",
    "print(f\"  Imagen shape : {img_sample.shape}\")\n",
    "print(f\"  Num bboxes   : {len(boxes_sample)}\")\n",
    "print(f\"  Campos clave : {list(keys_sample.keys())}\")\n",
    "print(f\"  Valores      : {keys_sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505983b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Distribución de campos clave en todo el dataset ─────────────────────────\n",
    "field_counts = {f: 0 for f in FIELD_CLASSES if f != \"background\"}\n",
    "missing       = {f: 0 for f in field_counts}\n",
    "\n",
    "for idx in all_indices:\n",
    "    key_path = os.path.join(KEY_DIR, f\"{idx}.json\")\n",
    "    try:\n",
    "        with open(key_path) as f:\n",
    "            keys = json.load(f)\n",
    "        for field in field_counts:\n",
    "            if keys.get(field):\n",
    "                field_counts[field] += 1\n",
    "            else:\n",
    "                missing[field] += 1\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "axes[0].bar(field_counts.keys(), field_counts.values(), color=[\"#E74C3C\",\"#2ECC71\",\"#3498DB\",\"#F39C12\"])\n",
    "axes[0].set_title(\"Muestras con cada campo clave\")\n",
    "axes[0].set_ylabel(\"Cantidad\")\n",
    "for i, (k, v) in enumerate(field_counts.items()):\n",
    "    axes[0].text(i, v + 2, str(v), ha=\"center\", fontsize=11)\n",
    "\n",
    "axes[1].bar(missing.keys(), missing.values(), color=\"#95A5A6\")\n",
    "axes[1].set_title(\"Muestras con campo ausente / vacío\")\n",
    "axes[1].set_ylabel(\"Cantidad\")\n",
    "for i, (k, v) in enumerate(missing.items()):\n",
    "    axes[1].text(i, v + 2, str(v), ha=\"center\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67930d2c",
   "metadata": {},
   "source": [
    "## 3. Visualizar Imágenes con Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db25c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_key_boxes(boxes_df: pd.DataFrame, keys: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Busca en boxes_df las filas cuyo texto coincide (parcialmente) con el\n",
    "    valor de cada campo clave. Retorna {field: list_of_row_indices}.\n",
    "    \"\"\"\n",
    "    matched = {f: [] for f in keys}\n",
    "    for field, value in keys.items():\n",
    "        if not value:\n",
    "            continue\n",
    "        # Búsqueda fuzzy: normalizar a minúsculas, buscar substring\n",
    "        value_norm = value.lower().strip()\n",
    "        for i, row in boxes_df.iterrows():\n",
    "            if str(row[\"text\"]).lower().strip() in value_norm or \\\n",
    "               value_norm in str(row[\"text\"]).lower().strip():\n",
    "                matched[field].append(i)\n",
    "    return matched\n",
    "\n",
    "\n",
    "def draw_annotated(img, boxes_df, keys, title=\"\"):\n",
    "    \"\"\"Dibuja todas las bboxes en gris y los campos clave en color.\"\"\"\n",
    "    color_map = {\"company\":\"#E74C3C\",\"date\":\"#2ECC71\",\"address\":\"#3498DB\",\"total\":\"#F39C12\"}\n",
    "    canvas = img.copy()\n",
    "    matched = find_key_boxes(boxes_df, keys)\n",
    "\n",
    "    # Pintar bboxes de texto genérico\n",
    "    for _, row in boxes_df.iterrows():\n",
    "        pts = np.array([[row.x1,row.y1],[row.x2,row.y2],\n",
    "                        [row.x3,row.y3],[row.x4,row.y4]], np.int32)\n",
    "        cv2.polylines(canvas, [pts.reshape(-1,1,2)], True, (180,180,180), 1)\n",
    "\n",
    "    # Pintar campos clave\n",
    "    for field, idxs in matched.items():\n",
    "        hex_c = color_map[field].lstrip(\"#\")\n",
    "        r,g,b = tuple(int(hex_c[i:i+2],16) for i in (0,2,4))\n",
    "        for i in idxs:\n",
    "            row = boxes_df.loc[i]\n",
    "            pts = np.array([[row.x1,row.y1],[row.x2,row.y2],\n",
    "                            [row.x3,row.y3],[row.x4,row.y4]], np.int32)\n",
    "            cv2.polylines(canvas,[pts.reshape(-1,1,2)],True,(r,g,b),3)\n",
    "            cv2.putText(canvas, field, (int(row.x1), int(row.y1)-5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.55, (r,g,b), 2)\n",
    "    return canvas\n",
    "\n",
    "\n",
    "# ─── Mostrar 6 muestras aleatorias ───────────────────────────────────────────\n",
    "np.random.seed(SEED)\n",
    "sample_ids = np.random.choice(all_indices, 6, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "for ax, idx in zip(axes.flat, sample_ids):\n",
    "    img, boxes_df, keys = load_sample(idx)\n",
    "    canvas = draw_annotated(img, boxes_df, keys, title=idx)\n",
    "    ax.imshow(canvas)\n",
    "    ax.set_title(f\"Recibo #{idx}\", fontsize=12)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Leyenda\n",
    "from matplotlib.patches import Patch\n",
    "legend_handles = [Patch(color=c, label=l) for l, c in\n",
    "                  [(\"company\",\"#E74C3C\"),(\"date\",\"#2ECC71\"),\n",
    "                   (\"address\",\"#3498DB\"),(\"total\",\"#F39C12\")]]\n",
    "fig.legend(handles=legend_handles, loc=\"lower center\", ncol=4, fontsize=12,\n",
    "           bbox_to_anchor=(0.5, -0.02))\n",
    "plt.suptitle(\"Recibos con campos clave destacados\", fontsize=15, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e6809",
   "metadata": {},
   "source": [
    "## 4. Preprocesar Imágenes y Generar Máscaras de Segmentación\n",
    "\n",
    "Cada imagen se redimensiona a `512×512`. Para cada bbox que coincide con un campo clave se dibuja en la máscara el polígono con el ID de clase correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78567b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_segmentation_mask(img_orig, boxes_df, keys):\n",
    "    \"\"\"\n",
    "    Genera una máscara de segmentación semántica del tamaño original de la imagen.\n",
    "    Cada píxel inside un bbox de campo clave recibe el ID de clase correspondiente.\n",
    "    \"\"\"\n",
    "    h, w = img_orig.shape[:2]\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)  # 0 = background\n",
    "\n",
    "    matched = find_key_boxes(boxes_df, keys)\n",
    "    for field, idxs in matched.items():\n",
    "        class_id = FIELD_CLASSES[field]\n",
    "        for i in idxs:\n",
    "            row = boxes_df.loc[i]\n",
    "            pts = np.array([[row.x1,row.y1],[row.x2,row.y2],\n",
    "                            [row.x3,row.y3],[row.x4,row.y4]], np.int32)\n",
    "            cv2.fillPoly(mask, [pts], class_id)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def preprocess_sample(idx):\n",
    "    \"\"\"Carga, redimensiona y devuelve (img_norm, mask_resized).\"\"\"\n",
    "    img, boxes_df, keys = load_sample(idx)\n",
    "    mask = build_segmentation_mask(img, boxes_df, keys)\n",
    "\n",
    "    img_resized  = cv2.resize(img,  (IMG_W, IMG_H), interpolation=cv2.INTER_LINEAR)\n",
    "    mask_resized = cv2.resize(mask, (IMG_W, IMG_H), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    img_norm = img_resized.astype(np.float32) / 255.0\n",
    "    return img_norm, mask_resized\n",
    "\n",
    "\n",
    "# ─── Verificación visual de las máscaras ─────────────────────────────────────\n",
    "cmap = ListedColormap([\"black\",\"red\",\"lime\",\"blue\",\"yellow\"])\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "for i, idx in enumerate(sample_ids[:3]):\n",
    "    img_n, mask = preprocess_sample(idx)\n",
    "    axes[i][0].imshow(img_n);                axes[i][0].set_title(f\"#{idx} — Imagen\")\n",
    "    axes[i][1].imshow(mask, cmap=cmap, vmin=0, vmax=4)\n",
    "    axes[i][1].set_title(\"Máscara de segmentación\")\n",
    "    axes[i][2].imshow(img_n)\n",
    "    axes[i][2].imshow(mask, cmap=cmap, vmin=0, vmax=4, alpha=0.45)\n",
    "    axes[i][2].set_title(\"Overlay\")\n",
    "    for ax in axes[i]: ax.axis(\"off\")\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "handles = [Patch(color=c, label=n) for c, n in\n",
    "           zip([\"black\",\"red\",\"lime\",\"blue\",\"yellow\"], CLASS_NAMES)]\n",
    "fig.legend(handles=handles, loc=\"lower center\", ncol=5, fontsize=11,\n",
    "           bbox_to_anchor=(0.5, -0.01))\n",
    "plt.suptitle(\"Imagen | Máscara | Overlay\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b93959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Dividir dataset en train / val / test (70 / 15 / 15) ───────────────────\n",
    "train_ids, temp_ids = train_test_split(all_indices, test_size=0.30, random_state=SEED)\n",
    "val_ids,   test_ids = train_test_split(temp_ids,    test_size=0.50, random_state=SEED)\n",
    "\n",
    "print(f\"Train : {len(train_ids)} muestras\")\n",
    "print(f\"Val   : {len(val_ids)}   muestras\")\n",
    "print(f\"Test  : {len(test_ids)}  muestras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572c6cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── tf.data pipeline ────────────────────────────────────────────────────────\n",
    "def load_tf_sample(idx_tensor):\n",
    "    idx = idx_tensor.numpy().decode(\"utf-8\")\n",
    "    img, mask = preprocess_sample(idx)\n",
    "    return img.astype(np.float32), mask.astype(np.int32)\n",
    "\n",
    "def tf_load(idx):\n",
    "    img, mask = tf.py_function(load_tf_sample, [idx], [tf.float32, tf.int32])\n",
    "    img.set_shape([IMG_H, IMG_W, 3])\n",
    "    mask.set_shape([IMG_H, IMG_W])\n",
    "    return img, mask\n",
    "\n",
    "def make_dataset(ids, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(ids)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(len(ids), seed=SEED)\n",
    "    ds = ds.map(tf_load, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "ds_train = make_dataset(train_ids, shuffle=True)\n",
    "ds_val   = make_dataset(val_ids,   shuffle=False)\n",
    "ds_test  = make_dataset(test_ids,  shuffle=False)\n",
    "\n",
    "print(\"Datasets creados:\")\n",
    "print(f\"  ds_train → {ds_train}\")\n",
    "print(f\"  ds_val   → {ds_val}\")\n",
    "print(f\"  ds_test  → {ds_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4740c018",
   "metadata": {},
   "source": [
    "## 5. Arquitectura del Modelo — U-Net\n",
    "\n",
    "Se utiliza **U-Net** con un encoder basado en **MobileNetV2** preentrenado en ImageNet. El decoder reconstruye la resolución mediante bloques de upsampling + convolución, concatenando los skip connections del encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a2ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_block(x, skip, filters, name):\n",
    "    x = layers.UpSampling2D(2, name=f\"{name}_up\")(x)\n",
    "    x = layers.Concatenate(name=f\"{name}_concat\")([x, skip])\n",
    "    x = layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\", name=f\"{name}_conv1\")(x)\n",
    "    x = layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\", name=f\"{name}_conv2\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_unet(input_shape=(IMG_H, IMG_W, 3), num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    U-Net con encoder MobileNetV2 (pretrained ImageNet) y decoder personalizado.\n",
    "    \"\"\"\n",
    "    inp = layers.Input(shape=input_shape, name=\"input_image\")\n",
    "\n",
    "    # ── Encoder: MobileNetV2 ─────────────────────────────────────────────────\n",
    "    base = keras.applications.MobileNetV2(\n",
    "        input_tensor=inp, include_top=False, weights=\"imagenet\"\n",
    "    )\n",
    "    base.trainable = False  # Congelar encoder inicialmente\n",
    "\n",
    "    # Skip connections en resoluciones intermedias\n",
    "    s1 = base.get_layer(\"block_1_expand_relu\").output   # 256×256\n",
    "    s2 = base.get_layer(\"block_3_expand_relu\").output   # 128×128\n",
    "    s3 = base.get_layer(\"block_6_expand_relu\").output   # 64×64\n",
    "    s4 = base.get_layer(\"block_13_expand_relu\").output  # 32×32\n",
    "    bridge = base.get_layer(\"block_16_project\").output  # 16×16\n",
    "\n",
    "    # ── Decoder ───────────────────────────────────────────────────────────────\n",
    "    d1 = upsample_block(bridge, s4, 512, \"dec1\")   # → 32×32\n",
    "    d2 = upsample_block(d1,     s3, 256, \"dec2\")   # → 64×64\n",
    "    d3 = upsample_block(d2,     s2, 128, \"dec3\")   # → 128×128\n",
    "    d4 = upsample_block(d3,     s1,  64, \"dec4\")   # → 256×256\n",
    "\n",
    "    # Última upsample para llegar a 512×512\n",
    "    x  = layers.UpSampling2D(2, name=\"final_up\")(d4)\n",
    "    x  = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\", name=\"final_conv\")(x)\n",
    "\n",
    "    # Capa de salida — una probabilidad por clase por píxel\n",
    "    out = layers.Conv2D(num_classes, 1, activation=\"softmax\", name=\"output_mask\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out, name=\"UNet_MobileNetV2\")\n",
    "    return model\n",
    "\n",
    "\n",
    "unet = build_unet()\n",
    "unet.summary(line_length=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d34e00",
   "metadata": {},
   "source": [
    "## 6. Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8629b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Función de pérdida: Sparse Categorical Crossentropy + pesos por clase ───\n",
    "# Las clases 1-4 son minoritarias vs background → se les da mayor peso\n",
    "class_weights = np.array([0.1, 2.0, 2.0, 2.0, 2.0], dtype=np.float32)\n",
    "\n",
    "def weighted_sparse_cce(y_true, y_pred):\n",
    "    scce   = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    # Convertir etiquetas en pesos\n",
    "    w_map  = tf.gather(class_weights, tf.cast(y_true, tf.int32))\n",
    "    loss   = scce * w_map\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "# ─── Métrica: Mean IoU ────────────────────────────────────────────────────────\n",
    "mean_iou = tf.keras.metrics.MeanIoU(num_classes=NUM_CLASSES, name=\"mean_iou\")\n",
    "\n",
    "\n",
    "# ─── Compilar ─────────────────────────────────────────────────────────────────\n",
    "unet.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=weighted_sparse_cce,\n",
    "    metrics=[\"accuracy\", mean_iou]\n",
    ")\n",
    "\n",
    "# ─── Callbacks ────────────────────────────────────────────────────────────────\n",
    "CKPT_PATH = os.path.join(BASE_DIR, \"best_unet.keras\")\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(CKPT_PATH, monitor=\"val_mean_iou\", mode=\"max\",\n",
    "                    save_best_only=True, verbose=1),\n",
    "    EarlyStopping(monitor=\"val_mean_iou\", mode=\"max\", patience=7,\n",
    "                  restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3,\n",
    "                      min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"Modelo compilado. Iniciando entrenamiento...\")\n",
    "history = unet.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_val,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a48737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Fine-tuning: descongelar encoder y entrenar con LR bajo ─────────────────\n",
    "print(\"=== Fine-tuning: descongelando encoder ===\")\n",
    "unet.get_layer(\"mobilenetv2_1.00_512\").trainable = True\n",
    "\n",
    "unet.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),\n",
    "    loss=weighted_sparse_cce,\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.MeanIoU(num_classes=NUM_CLASSES, name=\"mean_iou\")]\n",
    ")\n",
    "\n",
    "history_ft = unet.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_val,\n",
    "    epochs=15,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe76560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Curvas de entrenamiento ─────────────────────────────────────────────────\n",
    "def plot_history(hist, title=\"\"):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(17, 4))\n",
    "    for ax, metric, label in zip(axes,\n",
    "                                  [\"loss\",      \"accuracy\",   \"mean_iou\"],\n",
    "                                  [\"Loss\",      \"Accuracy\",   \"Mean IoU\"]):\n",
    "        ax.plot(hist.history[metric],     label=\"Train\")\n",
    "        ax.plot(hist.history[f\"val_{metric}\"], label=\"Val\", linestyle=\"--\")\n",
    "        ax.set_title(f\"{title} — {label}\")\n",
    "        ax.set_xlabel(\"Época\"); ax.legend()\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_history(history,    \"Fase 1 (encoder congelado)\")\n",
    "plot_history(history_ft, \"Fase 2 (fine-tuning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ead133",
   "metadata": {},
   "source": [
    "## 7. Evaluación del Modelo\n",
    "\n",
    "Se calculan **Mean IoU por clase**, **Pixel Accuracy** y **Precision / Recall / F1** sobre el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eadfa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Evaluar en test set ─────────────────────────────────────────────────────\n",
    "print(\"Evaluando en test set...\")\n",
    "results = unet.evaluate(ds_test, verbose=1)\n",
    "print(f\"\\n  Loss         : {results[0]:.4f}\")\n",
    "print(f\"  Pixel Acc    : {results[1]:.4f}\")\n",
    "print(f\"  Mean IoU     : {results[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c704157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── IoU por clase + reporte de clasificación ────────────────────────────────\n",
    "y_true_all, y_pred_all = [], []\n",
    "\n",
    "for imgs, masks in ds_test:\n",
    "    preds  = unet.predict(imgs, verbose=0)\n",
    "    pred_classes = np.argmax(preds, axis=-1)      # (B, H, W)\n",
    "    y_true_all.append(masks.numpy().flatten())\n",
    "    y_pred_all.append(pred_classes.flatten())\n",
    "\n",
    "y_true_flat = np.concatenate(y_true_all)\n",
    "y_pred_flat = np.concatenate(y_pred_all)\n",
    "\n",
    "# IoU por clase\n",
    "iou_metric = tf.keras.metrics.MeanIoU(num_classes=NUM_CLASSES)\n",
    "iou_metric.update_state(y_true_flat, y_pred_flat)\n",
    "cm = iou_metric.total_cm.numpy()\n",
    "\n",
    "iou_per_class = {}\n",
    "for i in range(NUM_CLASSES):\n",
    "    tp = cm[i, i]\n",
    "    fn = cm[i, :].sum() - tp\n",
    "    fp = cm[:, i].sum() - tp\n",
    "    denom = tp + fp + fn\n",
    "    iou_per_class[CLASS_NAMES[i]] = tp / denom if denom > 0 else 0.0\n",
    "\n",
    "print(\"IoU por clase:\")\n",
    "for cls, iou in iou_per_class.items():\n",
    "    print(f\"  {cls:>12s} : {iou:.4f}\")\n",
    "print(f\"\\n  Mean IoU  : {np.mean(list(iou_per_class.values())):.4f}\")\n",
    "\n",
    "print(\"\\nReporte de clasificación (nivel píxel):\")\n",
    "print(classification_report(y_true_flat, y_pred_flat,\n",
    "                             target_names=CLASS_NAMES, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3494bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Matriz de confusión (normalizada) ────────────────────────────────────────\n",
    "cm_norm = cm.astype(float)\n",
    "row_sums = cm_norm.sum(axis=1, keepdims=True)\n",
    "row_sums[row_sums == 0] = 1\n",
    "cm_norm /= row_sums\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "im = ax.imshow(cm_norm, cmap=\"Blues\", vmin=0, vmax=1)\n",
    "ax.set_xticks(range(NUM_CLASSES)); ax.set_xticklabels(CLASS_NAMES, rotation=30, ha=\"right\")\n",
    "ax.set_yticks(range(NUM_CLASSES)); ax.set_yticklabels(CLASS_NAMES)\n",
    "ax.set_xlabel(\"Predicción\"); ax.set_ylabel(\"Real\")\n",
    "ax.set_title(\"Matriz de confusión normalizada (nivel píxel)\")\n",
    "plt.colorbar(im, ax=ax)\n",
    "for i in range(NUM_CLASSES):\n",
    "    for j in range(NUM_CLASSES):\n",
    "        ax.text(j, i, f\"{cm_norm[i,j]:.2f}\", ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm[i,j] > 0.5 else \"black\", fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334cd686",
   "metadata": {},
   "source": [
    "## 8. Inferencia en Imágenes Nuevas\n",
    "\n",
    "Se carga el mejor checkpoint, se realizan predicciones y se visualizan los campos detectados con sus etiquetas y el score de confianza promedio por región."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798dd3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cargar el mejor modelo guardado ─────────────────────────────────────────\n",
    "best_model = keras.models.load_model(\n",
    "    CKPT_PATH,\n",
    "    custom_objects={\"weighted_sparse_cce\": weighted_sparse_cce}\n",
    ")\n",
    "print(f\"Modelo cargado desde: {CKPT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e463f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_visualize(idx, model, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predice la máscara de segmentación para un idx y visualiza:\n",
    "    - Imagen original\n",
    "    - Máscara predicha\n",
    "    - Overlay con contornos de campos detectados y confianza promedio\n",
    "    \"\"\"\n",
    "    img, boxes_df, keys = load_sample(idx)\n",
    "    img_resized = cv2.resize(img, (IMG_W, IMG_H))\n",
    "    img_norm    = img_resized.astype(np.float32) / 255.0\n",
    "\n",
    "    # Predicción\n",
    "    inp   = np.expand_dims(img_norm, 0)   # (1, H, W, 3)\n",
    "    probs = model.predict(inp, verbose=0)[0]   # (H, W, C)\n",
    "    pred_mask = np.argmax(probs, axis=-1)       # (H, W)\n",
    "\n",
    "    # Overlay con contornos por clase\n",
    "    overlay = img_resized.copy()\n",
    "    color_bgr = {1:(255,60,60), 2:(60,220,60), 3:(60,60,255), 4:(220,200,0)}\n",
    "\n",
    "    for cls_id, color in color_bgr.items():\n",
    "        cls_mask = (pred_mask == cls_id).astype(np.uint8) * 255\n",
    "        if cls_mask.max() == 0:\n",
    "            continue\n",
    "        contours, _ = cv2.findContours(cls_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cv2.drawContours(overlay, contours, -1, color, 3)\n",
    "        # Confianza promedio en la región\n",
    "        conf = probs[:,:,cls_id][pred_mask == cls_id].mean()\n",
    "        for cnt in contours:\n",
    "            x, y, w_c, h_c = cv2.boundingRect(cnt)\n",
    "            label_text = f\"{CLASS_NAMES[cls_id]} {conf:.2f}\"\n",
    "            cv2.putText(overlay, label_text, (x, max(y-6, 10)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.55, color, 2)\n",
    "\n",
    "    # Figura\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(17, 6))\n",
    "    axes[0].imshow(img_resized);         axes[0].set_title(f\"Original #{idx}\")\n",
    "    axes[1].imshow(pred_mask, cmap=cmap, vmin=0, vmax=4)\n",
    "    axes[1].set_title(\"Máscara predicha\")\n",
    "    axes[2].imshow(overlay);             axes[2].set_title(\"Campos detectados\")\n",
    "    for ax in axes: ax.axis(\"off\")\n",
    "\n",
    "    from matplotlib.patches import Patch\n",
    "    handles = [Patch(color=c, label=n) for c, n in\n",
    "               zip([\"red\",\"lime\",\"blue\",\"yellow\"], CLASS_NAMES[1:])]\n",
    "    fig.legend(handles=handles, loc=\"lower center\", ncol=4, fontsize=11,\n",
    "               bbox_to_anchor=(0.5, -0.01))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return pred_mask, probs\n",
    "\n",
    "\n",
    "# ─── Correr inferencia en 4 muestras del test set ─────────────────────────────\n",
    "np.random.seed(99)\n",
    "infer_ids = np.random.choice(test_ids, 4, replace=False)\n",
    "\n",
    "for idx in infer_ids:\n",
    "    predict_and_visualize(idx, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c51f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Inferencia en una imagen externa (fuera del dataset) ────────────────────\n",
    "# Reemplaza la ruta por una imagen de recibo propia si deseas probar\n",
    "def predict_external(image_path, model):\n",
    "    img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img, (IMG_W, IMG_H))\n",
    "    img_norm    = img_resized.astype(np.float32) / 255.0\n",
    "    inp  = np.expand_dims(img_norm, 0)\n",
    "    probs     = model.predict(inp, verbose=0)[0]\n",
    "    pred_mask = np.argmax(probs, axis=-1)\n",
    "\n",
    "    overlay = img_resized.copy()\n",
    "    color_bgr = {1:(255,60,60), 2:(60,220,60), 3:(60,60,255), 4:(220,200,0)}\n",
    "    for cls_id, color in color_bgr.items():\n",
    "        cls_mask = (pred_mask == cls_id).astype(np.uint8) * 255\n",
    "        if cls_mask.max() == 0:\n",
    "            continue\n",
    "        contours, _ = cv2.findContours(cls_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cv2.drawContours(overlay, contours, -1, color, 3)\n",
    "        conf = probs[:,:,cls_id][pred_mask == cls_id].mean()\n",
    "        for cnt in contours:\n",
    "            x, y, w_c, h_c = cv2.boundingRect(cnt)\n",
    "            cv2.putText(overlay, f\"{CLASS_NAMES[cls_id]} {conf:.2f}\", (x, max(y-6,10)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axes[0].imshow(img_resized); axes[0].set_title(\"Imagen de entrada\")\n",
    "    axes[1].imshow(overlay);     axes[1].set_title(\"Campos detectados\")\n",
    "    for ax in axes: ax.axis(\"off\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Extraer texto del campo total como ejemplo\n",
    "    for cls_id in range(1, NUM_CLASSES):\n",
    "        region = pred_mask == cls_id\n",
    "        coverage = region.sum() / region.size\n",
    "        print(f\"  {CLASS_NAMES[cls_id]:>12s}: cobertura={coverage*100:.2f}%  \"\n",
    "              f\"conf_media={probs[:,:,cls_id][region].mean() if region.any() else 0:.3f}\")\n",
    "\n",
    "\n",
    "# Ejemplo de uso — descomenta y ajusta la ruta:\n",
    "# predict_external(r\"C:\\ruta\\a\\recibo_nuevo.jpg\", best_model)\n",
    "print(\"Celda lista. Descomenta predict_external(...) con tu propia imagen.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
